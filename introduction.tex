\chapter{Introduction}


The adoption of cloud computing paradigm has changed the way we look at server infrastructure for next-generation applications. The cloud computing model has catalyzed a change from the traditional model of deploying applications on dedicated servers with limited room to scale to a new model of deploying applications on a shared pool of computing resources with (theoretically) unlimited scalability \cite{unlimited_scale}.  

The technology backbone behind the idea of cloud computing is virtualization. Virtualization is the process of creating virtual devices that simulate the hardware which are in turn mapped to the physical resources on an underlying server. Virtualization primarily addresses the problem of under-utilization of computing resources in the dedicated server model. Virtualization maximizes the utilization of the hardware investment by running an increased number of isolated applications simultaneously on a physical server or "\textit{host}". The isolated applications are run in an operating environment referred as \textit{Virtual Machines (VM)} \cite{5346711} or \textit{Containers} \cite{lxc}. The VMs / containers abstract the hardware interfaces required by the applications they run and utilize as much computational resources as they need (or are available). Virtualization opens the door for several added benefits:

    \textbf{Improved Fault-tolerance} :-  Virtual machines can be statefully migrated to other physical machines in the event of a hardware failure \cite{live_migration}. The fail-over can also be automatically triggered in some cluster-aware virtualization platforms  \cite{auto_vm_migration}. This facility effectively enables continuous availability for critical applications, which would require application-level awareness to achieve in the dedicated server model.

    \textbf{Operational Flexibility} :- Virtual Machines and their associated virtual devices are usually represented as a few files, which make them easy to clone, snapshot, and migrate to other physical machines. Also, individual VMs can be dynamically started or stopped without causing any outage to other VMs or the host.    

    \textbf{New Avenues for Tuning and Customization} :-  Since virtualization platforms introduce an additional layer of control between applications and hardware, they enable more options to customize the environment for individual VMs. That also provide more points of control to administer the resources accessible to individual virtual machines.

    \textbf{Granular Monitoring} :- The physical server that hosts VMs provides deeper insights and visibility into the performance, capacity, utilization and the overall health of the individual VMs. Analysis of the monitoring data facilitates resource management and control.

The benefits offered by the virtualization platforms form the core of the cloud computing ecosystem. Commercial cloud infrastructure providers have built their solutions around these benefits and offer "pay as you go" services where end-users are billed only for the resources used by the virtual machines or containers. Cloud providers pass on isolation and granular control options for the virtualization platform to end-users with a flexible interface, letting users keep complete control of their operating systems, storage environment, and networking setup without worrying about the underlying physical infrastructure. Large scale cloud computing platforms that lease their servers to virtualize applications of several end-users over a large pool of shared resources are exemplified by Amazon Elastic Compute Cloud (EC2) [cite], RackSpace [cite], and Heroku [cite].They differ by their choice of the virtualization platform. Enterprises and academic institutions also run a private cloud platform and have driven the development of open source cloud computing toolkits like OpenStack [cite], CloudStack [cite] and OpenShift [cite]. 

\section{Motivation}

The Intelligent River\textsuperscript{\textregistered} is a large scale environmental monitoring platform that is being built by researchers at Clemson University [cite]. The Intelligent River\textsuperscript{\textregistered} involves a large and distributed sensor network in the Savannah River basin and a real-time distributed middleware system hosted in Clemson University that receive, analyze and visualize the real-time environmental observation streams. The volume and unpredictable nature of the observation streams along with the increasing scale of sensor deployments demanded a distributed middleware system that is flexible, fault-tolerant and designed for scale. Architecting he Intelligent River\textsuperscript{\textregistered} middleware system posed an important design question,


\emph{Given the multitude of open source virtualization platforms, KVM, Xen, and Linux Containers, and the complex operational requirements of our middleware stack, which platform to choose ?}


Our quest to find the answer to the question needed a detailed analysis of the common virtualization platforms beyond the available results of the standard benchmarks [cite]. Prior research work by Andew J.Younge et al. [cite] on analysis of the virtualization platforms for HPC applications shows that KVM performed better compared to Xen on the standard HPCC benchmarks whereas another research work published by Jianhua Che et al. [cite] using the standard benchmarks claims that OpenVZ, a virtualization platform based on Linux containers performed the best while KVM performed \textit{significantly lower} than Xen which made clear that the comparison using the standard benchmarks were not enough. This motivated us to perform a detailed study of the principles behind the three major open source virtualization platforms, KVM, Xen, and Linux Containers and evaluate them based not just on the quantitative metrics like virtualization overhead and scalability but also on the qualitative metrics like operational flexibility, isolation, resource management, operational flexibility, and security. 

\section{Contributions}

This research thesis builds on the prior work on comparing the open source virtualization platforms and brings out the advantages and disadvantages of each.The contributions of this thesis include (i) Description of the scalable and resilient design of our Intelligent River\textsuperscript{\textregistered} middleware system. (ii) A study on the principles of operations behind the three chosen virtualization platforms which will be useful to the architects who design their applications around them. (iii) A quantitative comparison of the virtualization overhead (and scalability ?) exhibited by KVM, Xen and Linux containers as of date of writing. (iv) A discussion on the differences among the chosen platforms in terms of qualitative factors like ease of deployment, resiliency and security. (v) A discussion on the facilities to assure the resource entitlement and control with respect to CPU,Network bandwidth, Memory and I/O  which is often overlooked and an area in need of improvement from the operational perspective.
 
\section{Thesis Organization}

%\item How to architect our middleware stack to leverage the strengths of the chosen virtualization platform ? 

 
%Cloud computing has emerged as the model in which the end-user applications are deployed on top of a shared pool of computing resources.The core idea behind the cloud computing model is enabled by virtualization 
%\section{Cloud computing and Virtualization}

%Cloud computing has emerged as the model in which the end-user a			pplications are deployed on top of a shared pool of computing resources.The primary reasons behind the huge interest in the cloud computing model against the dedicated server model are : 

%(i) The computing resources on a dedicated server often goes under-utilized.
%(ii) 


%This model is enabled by deploying applications into \textit{virtual machines (VM)} or \textit{containers} instead of directly deploying them on the bare-metal systems.A large number of VMs can be hosted on a fewer number of physical machines.


%Virtualization is the creation of virtual resources from physical resources.The idea of virtualization originated in the 1960s as a way to partition the resources and run multiple applications simultaneously on IBM Mainframes(cite). The original proprietary design is still alive in an improved form in the name of z/VM and being used to run linux on the mainframes (cite). When the usage of x86 based servers increased in the early 2000s and the servers reported very low average utilization, the need for virtualization was realized and the hardware assist (cite) catalyzed the rapid growth of virtualization. Virtualization offers several benefits including greater hardware utilization, improved fault tolerance and recovery capabilities, automation, cloning and many more. 

%The core software component that enables virtualization is a \textit{hypervisor} which manages the mapping and control between the physical and virtual resources.The techniques used in the implementaion of such a mapping classifies virtualization into:

%1. Full virtualization (cite) - A complete simulation of the hardware to allow several guest operating systems to run in parallel. Example, Kernel-based Virtual Machines(KVM) (cite)


%2. Para virtualization (cite) - A partial simulation with communication between the guest operating systems and the hypervisor for critical kernel operations. Example, Xen (Cite)


%3. Container-based virtualization (cite) - An operating system level virtualization where multiple operating systems share the host's kernel in which they are isolated by name spaces (cite)

%Though aforesaid virtualization techniques enable great benefits in terms of granular resource control and better utilization of physical resources, they also exhibit an overhead in terms of CPU,Memory,Network and disk I/O.





      
%Wireless sensor network is a tool to visualize the physical world, We can say that they are our eyes. It is used to monitor volcano, structure of building or bridges, flow of water, temperature, humidity, soil moisture, etc. These are only few examples of what a wireless sensor network does for us. We gather data from these network, process and analyse it. This helps us in predicting things which would not have been possible earlier. By using this gathered data we can predict the lifetime of a bridge or weather conditions.
%Wireless sensor networks consist of small computing devices known as \textit{motes}, connected to each other wirelessly, in the hundreds to thousands of devices. Motes are equipped with sensors to sense physical and/or chemical parameters, and radios to transmit the sensed data to a base station for further processing and storage. These networks are important tools for monitoring the physical world. They can be used to monitor volcanoes \cite{bib_volcanic}, the structure of buildings and bridges \cite{bib_structural}, the flow of water \cite{bib_temp_humidity}, soil moisture \cite{bib_soil}, and other phenomena of interest. These are only a few examples of what wireless sensor networks can support. We can store, process, and analyze the data gathered from these networks to aid in analytical tasks that would not have been possible just a decade ago. Data gathered from a vibration sensor, for example, might be used to enable predictions regarding the lifetime of a bridge. Data gathered from an environmental network might be used for monitoring air pollution \cite{s8063601} or water quality \cite{white2010intelligent}. Data gathered from an earth monitoring network might be used for landslide detection \cite{5346714}.

%Wireless sensor network consists of small computing devices known as motes connected to each other wirelessly in numbers of hundreds and thousands. These motes have sensors on it to sense physical parameter, they transmit this sensed data to the base station for further processing. This motes are deployed in large geographical area and dangerous environment. These motes are low on processing power, memory and has to work in conditions which are not known in advance.
%\section{Motivation}
%The ideal wireless sensor network should be low power, maintenance free, scalable, reliable and inexpensive. But like every other system wireless sensor network has some limitations. The cost of making a network plays an important role. We should be able to build a mote which doesn't cost us much so that we can dispose it without thinking. This low cost of motes will help us in deploying more number of motes at a particular place and hence will provide us more precise data or can cover larger area in same cost. The problem which we normally face in wireless sensor network is of maintenance and most of the time maintenance is not of the components but is of battery which happens very frequently, changing a battery in a wireless sensor network is expensive and sometimes it is impossible to change batteries in every mote as they are deployed in large numbers and at dangerous places. Also reliable reception of data is important in wireless sensor network we cannot afford loss of data.
%Ideally, wireless sensor networks should be maintenance free, inexpensive, reliable, and scalable. But like every other type of system, wireless sensor networks have limitations. The first involves network maintenance, a significant obstacle. Most of the maintenance time is spent in replacing depleted batteries \cite{1607983}. Changing batteries in a large wireless sensor network can be expensive and time consuming. The cost of deploying a network also plays an important role. We should be able to deploy low cost, even disposable motes, in order to achieve better sensor coverage. We must also receive data reliably. Wireless sensor networks are often deployed in remote and perhaps in hostile environments, in large numbers \cite{1607983}. We cannot risk receiving incorrect data or missing important data. Finally, we should be able to accommodate the dynamic addition or unexpected removal of a mote in a network without affecting reliability.

%Our solution approach satisfies all of these requirements. We present the design of a mote that is maintenance free and inexpensive. We present a networking solution that is reliable and scalable.

% First, to develop a maintenance free mote. Second, to develop a inexpensive mote. Third, to develop a reliable network. Fourth to develop a scalable network.

%\section{Problem Statement}
%To solve the above limitations we have to develop a mote which is not expensive and has all the necessary features to sense and communicate the data to its peers. As maintenance of mote is an important issue, we have to build a mote which is maintenance free. Also we have to build a network which can reliably send and receive data, and a network which is scalable.
%The objective of this thesis is to develop a wireless sensor network that satisfies the stated key characteristics. First, the mote hardware must be maintenance free. To achieve a reduction in maintenance overhead, we must design a mote that harvests energy continuously. Second, the cost of each mote must be reduced to allow users to cover a large area at a reasonable cost. To achieve this reduction, we must develop a low-cost mote that provides the necessary components for sensing and transmitting data to a base station. Third, the network must be reliable. We deploy wireless sensor networks in remote, and perhaps even hostile environments. We cannot risk receiving incorrect data or losing data. Fourth, the network must be scalable. It should be able to accommodate the dynamic addition and unexpected removal of a mote without affecting overall reliability.

%\section{Solution Approach}
%Our solution approach consists of several steps. First, to achieve a reduction in maintenance time, our design includes an energy harvesting circuit, which harvests solar energy and stores it in a Li-Ion battery. This stored energy is used whenever there is insufficient solar power. Further, we know energy saved translates to longevity improvements, so we use custom software to keep the microcontroller in a sleep state whenever possible. Second, we focus on low-cost hardware to sense the physical and chemical parameters in a micro environment. We rely on components that are low cost, but still sufficient for a simplified mote design. Instead of using more common, costly radios such as the CC2420 (10.08 USD) \cite{bib_cc2420}, the CC2400 (8.77 USD) \cite{bib_cc2400}, and others, we use the RFM12 (5.56 USD) \cite{bib_rfm12}. This is an inexpensive radio, but has all the features of more popular radios, including high data-rate capabilities, variable transmission power, multi-channel operation, and long range . Further, the RFM12 doesn't require any external components to transmit and receive data, which simplifies the design, whereas the CC2420, and CC2400 each need many external components to function. A complex design and additional components increases the cost of manufacturing. We are reducing the cost of the mote by simplifying its design and using components which best satisfy our needs. Third, we develop network protocols to receive data reliably. To achieve this, we develop a protocol which uses a lightweight time-division multiple access approach (TDMA) \cite{chan2time}, with acknowledgments to ensure that every packet reaches the base station. Fourth, we develop a network that accommodates the dynamic addition and removal of motes without affecting network reliability. The resulting network typically operates at a 10\% duty-cycle, saving significant power. 
%\section{Contributions}
%The contribution of this thesis is a system which is low cost with all the necessary features and which doesn't require any maintenance. Also a network which is reliable, synced on the basis of time and scalable.


%--------------------------------------------------------------------------------------------------------------------
%This thesis describes four major contributions. First, we describe a maintenance free network. We accomplish this by harvesting solar energy and by efficiently using the available energy. Second, we describe the design of an inexpensive mote. We accomplish this by designing motes which use basic electronic components \textemdash resistors, capacitors, and diodes \textemdash instead of more complex integrated circuits. Third, we describe a reliable network design. We accomplish this by developing a lightweight TDMA protocol with acknowledgments to ensure reliable data transmission and reception. Fourth, we describe a scalable network design that accommodates the dynamic addition and removal of motes without affecting network reliability. In addition to the above contributions, we also develop a lightweight time synchronization protocol and route formation protocol.
%--------------------------------------------------------------------------------------------------------------------------


%This thesis contributes a simple, and low-cost hardware platform, with a simple, and light-weight software solution, for wireless sensor network. The hardware/software solution consists of four features. First, we describe a maintenance free network. We accomplish this by harvesting solar energy and by efficiently using the available energy. Second, we describe the design of an inexpensive mote. We accomplish this by designing motes which use basic electronic components \textemdash resistors, capacitors, and diodes \textemdash instead of more complex integrated circuits. Third, we describe a reliable network design. We accomplish this by developing a lightweight TDMA protocol with acknowledgments to ensure reliable data transmission and reception. Fourth, we describe a scalable network design that accommodates the dynamic addition and removal of motes without affecting network reliability. In addition to the above contributions, we also develop a lightweight time synchronization protocol and route formation protocol.

%This thesis describes a low-cost, maintenance-free sensor networking platform, supported by lightweight, yet reliable and scalable networking software. Design simplicity is a focal point throughout. The hardware/software solution exhibits four key characteristics. First, we describe a \textemdash maintenance free solution. We accomplish this by harvesting solar energy and by efficiently using the available energy. Second, we describe an \textemdash inexpensive solution. We accomplish this by designing a mote platform which uses basic electronic components \textemdash resistors, capacitors, and diodes \textemdash instead of more complex integrated circuits. Third, we describe a \textemdash reliable solution that ensures high yield, even in the presence of  intermittent and permanent device faults\footnote{The solution assumes that the underlying physical topology always accommodates a connected routing topology.}. We accomplish this by developing lightweight route formation, time synchronization, and TDMA protocols; the latter includes application-level acknowledgments to ensure reliable data transmission and reception. Finally, we describe a \textemdash scalable solution that accommodates the dynamic addition and removal of motes without affecting network reliability. The sum total of these characteristics yields a low-cost sensor networking solution that enables large-scale, long-lived network deployments \textemdash the contribution of this thesis.



%Our implementation of harvesting solar energy, technique of utilizing the diodes in specific ways to switch circuits to save power, and our hardware design choices to bring down the total cost and making the system maintenance free are the major contributions towards the design of the future sensor networks. The software that drives this efficient design introduces a new network formation technique and is proven to be scalable, reliable and self-healing in case of mote failures.

%\section{Thesis Organization}
%Chapter 2 surveys the most closely related work in the field of wireless sensor networks and solar energy harvesting. Chapter 3 describes the design of the hardware. Chapter 4 describes the design of the network software and associated protocols. Chapter 5 describes the results of our evaluation. Finally, chapter 6 presents a summary of contributions and conclusions.